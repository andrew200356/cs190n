{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e628aa0",
   "metadata": {},
   "source": [
    "# Using Trustee with the Fugu-Trained Model: Application and Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd12220",
   "metadata": {},
   "source": [
    "## Imports for pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0eaf464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122a64d8",
   "metadata": {},
   "source": [
    "## Constants (Directly imported from the puffer paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7561418",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DURATION = 180180\n",
    "PKT_BYTES = 1500\n",
    "MILLION = 1000000\n",
    "PAST_CHUNKS = 8\n",
    "FUTURE_CHUNKS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622283df",
   "metadata": {},
   "source": [
    "## Steps:\n",
    "    1. 2 CSVs video sent and video acked\n",
    "    2. Parse them to relevant data types\n",
    "    3. calculate the transmission time from sent and acked\n",
    "    4. Consider the past 8 chunks and pad the values if necessary\n",
    "    5. Create the examples for the future chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4003341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df = pd.read_csv('/mnt/md0/cs190n/video_sent.csv')\n",
    "acked_df = pd.read_csv('/mnt/md0/cs190n/video_acked.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a935bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time (ns GMT)</th>\n",
       "      <th>session_id</th>\n",
       "      <th>index</th>\n",
       "      <th>expt_id</th>\n",
       "      <th>channel</th>\n",
       "      <th>video_ts</th>\n",
       "      <th>format</th>\n",
       "      <th>size</th>\n",
       "      <th>ssim_index</th>\n",
       "      <th>cwnd</th>\n",
       "      <th>in_flight</th>\n",
       "      <th>min_rtt</th>\n",
       "      <th>rtt</th>\n",
       "      <th>delivery_rate</th>\n",
       "      <th>buffer</th>\n",
       "      <th>cum_rebuf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1732103987439000000</td>\n",
       "      <td>RhMF72kUi5Yin0hEqud4YQJiQ7UmZgNxk9YGk6A8UL0=</td>\n",
       "      <td>1</td>\n",
       "      <td>2216</td>\n",
       "      <td>abc</td>\n",
       "      <td>42209327160</td>\n",
       "      <td>1280x720-26</td>\n",
       "      <td>907262</td>\n",
       "      <td>0.949086</td>\n",
       "      <td>1090</td>\n",
       "      <td>0</td>\n",
       "      <td>43153</td>\n",
       "      <td>54763</td>\n",
       "      <td>1299205</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1732103987568000000</td>\n",
       "      <td>RhMF72kUi5Yin0hEqud4YQJiQ7UmZgNxk9YGk6A8UL0=</td>\n",
       "      <td>1</td>\n",
       "      <td>2216</td>\n",
       "      <td>abc</td>\n",
       "      <td>42209507340</td>\n",
       "      <td>1280x720-20</td>\n",
       "      <td>947098</td>\n",
       "      <td>0.982131</td>\n",
       "      <td>1096</td>\n",
       "      <td>0</td>\n",
       "      <td>43153</td>\n",
       "      <td>55420</td>\n",
       "      <td>11655452</td>\n",
       "      <td>2.002</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1732103987697000000</td>\n",
       "      <td>RhMF72kUi5Yin0hEqud4YQJiQ7UmZgNxk9YGk6A8UL0=</td>\n",
       "      <td>1</td>\n",
       "      <td>2216</td>\n",
       "      <td>abc</td>\n",
       "      <td>42209687520</td>\n",
       "      <td>1280x720-20</td>\n",
       "      <td>537024</td>\n",
       "      <td>0.989087</td>\n",
       "      <td>1010</td>\n",
       "      <td>0</td>\n",
       "      <td>43153</td>\n",
       "      <td>54114</td>\n",
       "      <td>11630610</td>\n",
       "      <td>3.931</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1732103987792000000</td>\n",
       "      <td>RhMF72kUi5Yin0hEqud4YQJiQ7UmZgNxk9YGk6A8UL0=</td>\n",
       "      <td>1</td>\n",
       "      <td>2216</td>\n",
       "      <td>abc</td>\n",
       "      <td>42209867700</td>\n",
       "      <td>1280x720-20</td>\n",
       "      <td>402094</td>\n",
       "      <td>0.989844</td>\n",
       "      <td>1060</td>\n",
       "      <td>14</td>\n",
       "      <td>43153</td>\n",
       "      <td>53672</td>\n",
       "      <td>11630610</td>\n",
       "      <td>5.836</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1732103987899000000</td>\n",
       "      <td>RhMF72kUi5Yin0hEqud4YQJiQ7UmZgNxk9YGk6A8UL0=</td>\n",
       "      <td>1</td>\n",
       "      <td>2216</td>\n",
       "      <td>abc</td>\n",
       "      <td>42210047880</td>\n",
       "      <td>1280x720-20</td>\n",
       "      <td>847411</td>\n",
       "      <td>0.987943</td>\n",
       "      <td>1060</td>\n",
       "      <td>0</td>\n",
       "      <td>43153</td>\n",
       "      <td>56265</td>\n",
       "      <td>11630610</td>\n",
       "      <td>7.733</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time (ns GMT)                                    session_id  index  \\\n",
       "0  1732103987439000000  RhMF72kUi5Yin0hEqud4YQJiQ7UmZgNxk9YGk6A8UL0=      1   \n",
       "1  1732103987568000000  RhMF72kUi5Yin0hEqud4YQJiQ7UmZgNxk9YGk6A8UL0=      1   \n",
       "2  1732103987697000000  RhMF72kUi5Yin0hEqud4YQJiQ7UmZgNxk9YGk6A8UL0=      1   \n",
       "3  1732103987792000000  RhMF72kUi5Yin0hEqud4YQJiQ7UmZgNxk9YGk6A8UL0=      1   \n",
       "4  1732103987899000000  RhMF72kUi5Yin0hEqud4YQJiQ7UmZgNxk9YGk6A8UL0=      1   \n",
       "\n",
       "   expt_id channel     video_ts       format    size  ssim_index  cwnd  \\\n",
       "0     2216     abc  42209327160  1280x720-26  907262    0.949086  1090   \n",
       "1     2216     abc  42209507340  1280x720-20  947098    0.982131  1096   \n",
       "2     2216     abc  42209687520  1280x720-20  537024    0.989087  1010   \n",
       "3     2216     abc  42209867700  1280x720-20  402094    0.989844  1060   \n",
       "4     2216     abc  42210047880  1280x720-20  847411    0.987943  1060   \n",
       "\n",
       "   in_flight  min_rtt    rtt  delivery_rate  buffer  cum_rebuf  \n",
       "0          0    43153  54763        1299205   0.000      0.000  \n",
       "1          0    43153  55420       11655452   2.002      0.000  \n",
       "2          0    43153  54114       11630610   3.931      0.166  \n",
       "3         14    43153  53672       11630610   5.836      0.166  \n",
       "4          0    43153  56265       11630610   7.733      0.166  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc2c7ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time (ns GMT)</th>\n",
       "      <th>session_id</th>\n",
       "      <th>index</th>\n",
       "      <th>expt_id</th>\n",
       "      <th>channel</th>\n",
       "      <th>video_ts</th>\n",
       "      <th>buffer</th>\n",
       "      <th>cum_rebuf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1732103987564000000</td>\n",
       "      <td>RhMF72kUi5Yin0hEqud4YQJiQ7UmZgNxk9YGk6A8UL0=</td>\n",
       "      <td>1</td>\n",
       "      <td>2216</td>\n",
       "      <td>abc</td>\n",
       "      <td>42209327160</td>\n",
       "      <td>2.002</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1732103987693000000</td>\n",
       "      <td>RhMF72kUi5Yin0hEqud4YQJiQ7UmZgNxk9YGk6A8UL0=</td>\n",
       "      <td>1</td>\n",
       "      <td>2216</td>\n",
       "      <td>abc</td>\n",
       "      <td>42209507340</td>\n",
       "      <td>3.931</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1732103987788000000</td>\n",
       "      <td>RhMF72kUi5Yin0hEqud4YQJiQ7UmZgNxk9YGk6A8UL0=</td>\n",
       "      <td>1</td>\n",
       "      <td>2216</td>\n",
       "      <td>abc</td>\n",
       "      <td>42209687520</td>\n",
       "      <td>5.836</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1732103987895000000</td>\n",
       "      <td>RhMF72kUi5Yin0hEqud4YQJiQ7UmZgNxk9YGk6A8UL0=</td>\n",
       "      <td>1</td>\n",
       "      <td>2216</td>\n",
       "      <td>abc</td>\n",
       "      <td>42209867700</td>\n",
       "      <td>7.733</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1732103988019000000</td>\n",
       "      <td>RhMF72kUi5Yin0hEqud4YQJiQ7UmZgNxk9YGk6A8UL0=</td>\n",
       "      <td>1</td>\n",
       "      <td>2216</td>\n",
       "      <td>abc</td>\n",
       "      <td>42210047880</td>\n",
       "      <td>9.613</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time (ns GMT)                                    session_id  index  \\\n",
       "0  1732103987564000000  RhMF72kUi5Yin0hEqud4YQJiQ7UmZgNxk9YGk6A8UL0=      1   \n",
       "1  1732103987693000000  RhMF72kUi5Yin0hEqud4YQJiQ7UmZgNxk9YGk6A8UL0=      1   \n",
       "2  1732103987788000000  RhMF72kUi5Yin0hEqud4YQJiQ7UmZgNxk9YGk6A8UL0=      1   \n",
       "3  1732103987895000000  RhMF72kUi5Yin0hEqud4YQJiQ7UmZgNxk9YGk6A8UL0=      1   \n",
       "4  1732103988019000000  RhMF72kUi5Yin0hEqud4YQJiQ7UmZgNxk9YGk6A8UL0=      1   \n",
       "\n",
       "   expt_id channel     video_ts  buffer  cum_rebuf  \n",
       "0     2216     abc  42209327160   2.002      0.000  \n",
       "1     2216     abc  42209507340   3.931      0.166  \n",
       "2     2216     abc  42209687520   5.836      0.166  \n",
       "3     2216     abc  42209867700   7.733      0.166  \n",
       "4     2216     abc  42210047880   9.613      0.166  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acked_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d4cb378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_raw_data(video_sent_path, video_acked_path, time_start=None, time_end=None):\n",
    "    \"\"\"\n",
    "    Load data from files and calculate chunk transmission times.\n",
    "    \"\"\"\n",
    "    video_sent_df = pd.read_csv(video_sent_path)\n",
    "    video_acked_df = pd.read_csv(video_acked_path)\n",
    "    video_sent_df = video_sent_df.sort_values(by='video_ts')\n",
    "    video_acked_df = video_acked_df.sort_values(by='video_ts')\n",
    "\n",
    "    # Rename \"time (ns GMT)\" to \"time\" for convenience\n",
    "    video_sent_df.rename(columns={'time (ns GMT)': 'time'}, inplace=True)\n",
    "    video_acked_df.rename(columns={'time (ns GMT)': 'time'}, inplace=True)\n",
    "\n",
    "    # Convert nanosecond timestamps to datetime\n",
    "    video_sent_df['time'] = pd.to_datetime(video_sent_df['time'], unit='ns')\n",
    "    video_acked_df['time'] = pd.to_datetime(video_acked_df['time'], unit='ns')\n",
    "\n",
    "    # Filter by time range\n",
    "    if time_start:\n",
    "        time_start = pd.to_datetime(time_start)\n",
    "        video_sent_df = video_sent_df[video_sent_df['time'] >= time_start]\n",
    "        video_acked_df = video_acked_df[video_acked_df['time'] >= time_start]\n",
    "    if time_end:\n",
    "        time_end = pd.to_datetime(time_end)\n",
    "        video_sent_df = video_sent_df[video_sent_df['time'] <= time_end]\n",
    "        video_acked_df = video_acked_df[video_acked_df['time'] <= time_end]\n",
    "\n",
    "    # Process the data\n",
    "    return calculate_trans_times(video_sent_df, video_acked_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bcdb6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trans_times(video_sent_df, video_acked_df):\n",
    "    \"\"\"\n",
    "    Calculate transmission times from video_sent and video_acked datasets using session_id.\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    last_video_ts = {}\n",
    "\n",
    "    for _, row in video_sent_df.iterrows():\n",
    "        session = row['session_id']  # Use only session_id to track sessions\n",
    "        if session not in d:\n",
    "            d[session] = {}\n",
    "            last_video_ts[session] = None\n",
    "\n",
    "        video_ts = int(row['video_ts'])\n",
    "        if last_video_ts[session] is not None:\n",
    "            if video_ts != last_video_ts[session] + VIDEO_DURATION:\n",
    "                continue\n",
    "\n",
    "        last_video_ts[session] = video_ts\n",
    "        d[session][video_ts] = {\n",
    "            'sent_ts': pd.Timestamp(row['time']),\n",
    "            'size': float(row['size']) / PKT_BYTES,\n",
    "            'delivery_rate': float(row['delivery_rate']) / PKT_BYTES,\n",
    "            'cwnd': float(row['cwnd']),\n",
    "            'in_flight': float(row['in_flight']),\n",
    "            'min_rtt': float(row['min_rtt']) / MILLION,\n",
    "            'rtt': float(row['rtt']) / MILLION,\n",
    "        }\n",
    "\n",
    "    for _, row in video_acked_df.iterrows():\n",
    "        session = row['session_id']  # Use only session_id\n",
    "        if session not in d:\n",
    "            continue\n",
    "\n",
    "        video_ts = int(row['video_ts'])\n",
    "        if video_ts not in d[session]:\n",
    "            continue\n",
    "\n",
    "        dsv = d[session][video_ts]\n",
    "        sent_ts = dsv['sent_ts']\n",
    "        acked_ts = pd.Timestamp(row['time'])\n",
    "        dsv['acked_ts'] = acked_ts\n",
    "        dsv['trans_time'] = (acked_ts - sent_ts).total_seconds()\n",
    "    for session in d.keys():\n",
    "        d[session] = {\n",
    "            video_ts: data\n",
    "            for video_ts, data in d[session].items()\n",
    "            if data.get('trans_time', 0) >= 0  # Retain only non-negative trans_time\n",
    "        }\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6791a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = prepare_raw_data(\"/mnt/md0/cs190n/video_sent.csv\", \"/mnt/md0/cs190n/video_acked.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c3ee73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_past_chunks(ds, next_ts, row):\n",
    "    i = 1\n",
    "    past_chunks = []\n",
    "    while i <= PAST_CHUNKS:\n",
    "        ts = next_ts - i * VIDEO_DURATION\n",
    "        if ts in ds and 'trans_time' in ds[ts]:\n",
    "            past_chunks = [ds[ts]['delivery_rate'],\n",
    "                           ds[ts]['cwnd'], ds[ts]['in_flight'],\n",
    "                           ds[ts]['min_rtt'], ds[ts]['rtt'],\n",
    "                           ds[ts]['size'], ds[ts]['trans_time']] + past_chunks\n",
    "        else:\n",
    "            nts = ts + VIDEO_DURATION  # padding with the nearest ts\n",
    "            padding = [ds[nts]['delivery_rate'],\n",
    "                       ds[nts]['cwnd'], ds[nts]['in_flight'],\n",
    "                       ds[nts]['min_rtt'], ds[nts]['rtt']]\n",
    "            if nts == next_ts:\n",
    "                padding += [0, 0]  # next_ts is the first chunk to send\n",
    "            else:\n",
    "                padding += [ds[nts]['size'], ds[nts]['trans_time']]\n",
    "            break\n",
    "        i += 1\n",
    "    if i != PAST_CHUNKS + 1:  # break in the middle; padding must exist\n",
    "        while i <= PAST_CHUNKS:\n",
    "            past_chunks = padding + past_chunks\n",
    "            i += 1\n",
    "    row += past_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6b6741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_output(d):\n",
    "    ret = [{'in': [], 'out': []} for _ in range(5)]  # FUTURE_CHUNKS = 5\n",
    "\n",
    "    for session in d:\n",
    "        ds = d[session]\n",
    "\n",
    "        for next_ts in ds:\n",
    "            if 'trans_time' not in ds[next_ts]:\n",
    "                continue\n",
    "\n",
    "            row = []\n",
    "\n",
    "            # Append past chunks\n",
    "            append_past_chunks(ds, next_ts, row)\n",
    "\n",
    "            # Append the TCP info of the next chunk\n",
    "            row += [ds[next_ts]['delivery_rate'],\n",
    "                    ds[next_ts]['cwnd'], ds[next_ts]['in_flight'],\n",
    "                    ds[next_ts]['min_rtt'], ds[next_ts]['rtt']]\n",
    "\n",
    "            # Generate FUTURE_CHUNKS rows\n",
    "            for i in range(5):  # FUTURE_CHUNKS = 5\n",
    "                row_i = row.copy()\n",
    "\n",
    "                ts = next_ts + i * VIDEO_DURATION\n",
    "                if ts in ds and 'trans_time' in ds[ts]:\n",
    "                    row_i += [ds[ts]['size']]\n",
    "\n",
    "                    ret[i]['in'].append(row_i)\n",
    "                    ret[i]['out'].append(ds[ts]['trans_time'])\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fbf39386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_data(output_file, processed_data):\n",
    "    \"\"\"\n",
    "    Save processed data to a file.\n",
    "    \"\"\"\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(processed_data, f)\n",
    "    print(f\"Processed data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22170331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to /mnt/md0/laasya/output.pkl\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    DEFAULT_VIDEO_SENT_PATH = '/mnt/md0/cs190n/video_sent.csv'\n",
    "    DEFAULT_VIDEO_ACKED_PATH = '/mnt/md0/cs190n/video_acked.csv'\n",
    "    DEFAULT_OUTPUT_FILE = '/mnt/md0/laasya/output.pkl'\n",
    "    \n",
    "    #Latest datasets can be found at https://puffer.stanford.edu/results/\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=\"Process video streaming datasets.\")\n",
    "    parser.add_argument('--video_sent_path', type=str, help='Path to the video_sent dataset CSV file.')\n",
    "    parser.add_argument('--video_acked_path', type=str, help='Path to the video_acked dataset CSV file.')\n",
    "    parser.add_argument('--output_file', type=str, help='Path to save the processed data.')\n",
    "    parser.add_argument('--time_start', type=str, default=None, help='Start time for filtering data (RFC3339 format).')\n",
    "    parser.add_argument('--time_end', type=str, default=None, help='End time for filtering data (RFC3339 format).')\n",
    "    #args = parser.parse_args()\n",
    "    #processed_data = prepare_input_output(prepare_raw_data(args.video_sent_path, args.video_acked_path,\n",
    "    #    time_start=args.time_start, time_end=args.time_end))\n",
    "    # save_processed_data(args.output_file, processed_data)\n",
    "    processed_data = prepare_input_output(prepare_raw_data(DEFAULT_VIDEO_SENT_PATH, DEFAULT_VIDEO_ACKED_PATH,\n",
    "        time_start=None, time_end=None))\n",
    "    save_processed_data(DEFAULT_OUTPUT_FILE, processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b31d438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_pickle(\"/mnt/md0/laasya/output.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "594f418c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.12, 0.136, 0.218, 0.063, 0.093]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[0][\"out\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36de8b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  0,\n",
       "  0,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  0,\n",
       "  0,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  0,\n",
       "  0,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  0,\n",
       "  0,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  0,\n",
       "  0,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  0,\n",
       "  0,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  0,\n",
       "  0,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  0,\n",
       "  0,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  10.856],\n",
       " [36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  10.856,\n",
       "  0.12,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  10.856,\n",
       "  0.12,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  10.856,\n",
       "  0.12,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  10.856,\n",
       "  0.12,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  10.856,\n",
       "  0.12,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  10.856,\n",
       "  0.12,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  10.856,\n",
       "  0.12,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  10.856,\n",
       "  0.12,\n",
       "  278.31666666666666,\n",
       "  42.0,\n",
       "  15.0,\n",
       "  0.043168,\n",
       "  0.055525,\n",
       "  68.61466666666666],\n",
       " [36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  10.856,\n",
       "  0.12,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  10.856,\n",
       "  0.12,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  10.856,\n",
       "  0.12,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  10.856,\n",
       "  0.12,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  10.856,\n",
       "  0.12,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  10.856,\n",
       "  0.12,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  10.856,\n",
       "  0.12,\n",
       "  278.31666666666666,\n",
       "  42.0,\n",
       "  15.0,\n",
       "  0.043168,\n",
       "  0.055525,\n",
       "  68.61466666666666,\n",
       "  0.136,\n",
       "  741.506,\n",
       "  102.0,\n",
       "  0.0,\n",
       "  0.043168,\n",
       "  0.054192,\n",
       "  370.19866666666667],\n",
       " [36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  10.856,\n",
       "  0.12,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  10.856,\n",
       "  0.12,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  10.856,\n",
       "  0.12,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  10.856,\n",
       "  0.12,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  10.856,\n",
       "  0.12,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  10.856,\n",
       "  0.12,\n",
       "  278.31666666666666,\n",
       "  42.0,\n",
       "  15.0,\n",
       "  0.043168,\n",
       "  0.055525,\n",
       "  68.61466666666666,\n",
       "  0.136,\n",
       "  741.506,\n",
       "  102.0,\n",
       "  0.0,\n",
       "  0.043168,\n",
       "  0.054192,\n",
       "  370.19866666666667,\n",
       "  0.218,\n",
       "  3100.468,\n",
       "  435.0,\n",
       "  0.0,\n",
       "  0.043153,\n",
       "  0.051418,\n",
       "  83.16266666666667],\n",
       " [36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  10.856,\n",
       "  0.12,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  10.856,\n",
       "  0.12,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  10.856,\n",
       "  0.12,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  10.856,\n",
       "  0.12,\n",
       "  36.882,\n",
       "  14.0,\n",
       "  0.0,\n",
       "  0.05278,\n",
       "  0.059686,\n",
       "  10.856,\n",
       "  0.12,\n",
       "  278.31666666666666,\n",
       "  42.0,\n",
       "  15.0,\n",
       "  0.043168,\n",
       "  0.055525,\n",
       "  68.61466666666666,\n",
       "  0.136,\n",
       "  741.506,\n",
       "  102.0,\n",
       "  0.0,\n",
       "  0.043168,\n",
       "  0.054192,\n",
       "  370.19866666666667,\n",
       "  0.218,\n",
       "  3100.468,\n",
       "  435.0,\n",
       "  0.0,\n",
       "  0.043153,\n",
       "  0.051418,\n",
       "  83.16266666666667,\n",
       "  0.063,\n",
       "  3100.468,\n",
       "  513.0,\n",
       "  0.0,\n",
       "  0.043153,\n",
       "  0.0533,\n",
       "  187.646]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[0][\"in\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafe3a6d",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "1. Define the model.\n",
    "2. Normalize the data\n",
    "3. Discretize the labels\n",
    "4. Plot the classification report\n",
    "5. Use trustee to identify the important features\n",
    "6. Plot the Trustee tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "995bbb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/laasya/.local/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /home/laasya/.local/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch) (3.0.3)\n",
      "Requirement already satisfied: fsspec in /home/laasya/.local/lib/python3.10/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/laasya/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/laasya/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/laasya/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/laasya/.local/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/laasya/.local/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/laasya/.local/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/laasya/.local/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/laasya/.local/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/laasya/.local/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/laasya/.local/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/laasya/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/laasya/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/laasya/.local/lib/python3.10/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/laasya/.local/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/laasya/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d79f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model side imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "from trustee import ClassificationTrustee\n",
    "import graphviz\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03b3e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 500\n",
    "DEVICE = torch.device('cpu')\n",
    "\n",
    "inference = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca1cb03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    #Model constants\n",
    "    PAST_CHUNKS = 8\n",
    "    FUTURE_CHUNKS = 5\n",
    "    DIM_IN = 62\n",
    "    COLUMNS = [j + str(i) for i in range(PAST_CHUNKS + 1) for j in ['delivery_rate', 'cwnd', 'in_flight', 'min_rtt', 'rtt', 'size', 'trans_time']][:DIM_IN]\n",
    "    DIM_OUT = 21  # BIN_MAX + 1\n",
    "    DIM_H1 = 64\n",
    "    DIM_H2 = 64\n",
    "    LEARNING_RATE = 1e-4\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    BIN_SIZE = 0.5  # seconds\n",
    "    BIN_MAX = 20\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(Model.DIM_IN, Model.DIM_H1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(Model.DIM_H1, Model.DIM_H2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(Model.DIM_H2, Model.DIM_OUT),\n",
    "        ).double().to(device=DEVICE)\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss().to(device=DEVICE)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(),\n",
    "                                          lr=Model.LEARNING_RATE,\n",
    "                                          weight_decay=Model.WEIGHT_DECAY)\n",
    "        self.obs_size = None\n",
    "        self.obs_mean = None\n",
    "        self.obs_std = None\n",
    "\n",
    "    def update_obs_stats(self, raw_in):\n",
    "        if self.obs_size is None:\n",
    "            self.obs_size = len(raw_in)\n",
    "            self.obs_mean = np.mean(raw_in, axis=0)\n",
    "            self.obs_std = np.std(raw_in, axis=0)\n",
    "            return\n",
    "        old_size = self.obs_size\n",
    "        new_size = len(raw_in)\n",
    "        self.obs_size = old_size + new_size\n",
    "        old_mean = self.obs_mean\n",
    "        new_mean = np.mean(raw_in, axis=0)\n",
    "        self.obs_mean = (old_mean * old_size + new_mean * new_size) / self.obs_size\n",
    "        old_std = self.obs_std\n",
    "        old_sum_square = old_size * (np.square(old_std) + np.square(old_mean))\n",
    "        new_sum_square = np.sum(np.square(raw_in), axis=0)\n",
    "        mean_square = (old_sum_square + new_sum_square) / self.obs_size\n",
    "        self.obs_std = np.sqrt(mean_square - np.square(self.obs_mean))\n",
    "\n",
    "    def normalize_input(self, raw_in, update_obs=False):\n",
    "        z = np.array(raw_in)\n",
    "        if update_obs:\n",
    "            self.update_obs_stats(z)\n",
    "        assert self.obs_size is not None\n",
    "        for col in range(len(self.obs_mean)):\n",
    "            z[:, col] -= self.obs_mean[col]\n",
    "            if self.obs_std[col] != 0:\n",
    "                z[:, col] /= self.obs_std[col]\n",
    "        return z\n",
    "\n",
    "    def discretize_output(self, raw_out):\n",
    "        z = np.array(raw_out)\n",
    "        z = np.floor((z + 0.5 * Model.BIN_SIZE) / Model.BIN_SIZE).astype(int)\n",
    "        return np.clip(z, 0, Model.BIN_MAX)\n",
    "\n",
    "    def train(self, train_input, train_output, test_input, test_output):\n",
    "        train_input = torch.from_numpy(self.normalize_input(train_input, update_obs=inference)).to(DEVICE)\n",
    "        train_output = torch.from_numpy(self.discretize_output(train_output)).to(DEVICE)\n",
    "        test_input = torch.from_numpy(self.normalize_input(test_input, update_obs=False)).to(DEVICE)\n",
    "        test_output = torch.from_numpy(self.discretize_output(test_output)).to(DEVICE)\n",
    "\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            self.model.train()\n",
    "            perm = np.random.permutation(len(train_input))\n",
    "            train_input = train_input[perm]\n",
    "            train_output = train_output[perm]\n",
    "\n",
    "            num_batches = int(np.ceil(len(train_input) / BATCH_SIZE))\n",
    "            epoch_loss = 0\n",
    "\n",
    "            for i in range(num_batches):\n",
    "                start_idx = i * BATCH_SIZE\n",
    "                end_idx = min((i + 1) * BATCH_SIZE, len(train_input))\n",
    "\n",
    "                batch_input = train_input[start_idx:end_idx]\n",
    "                batch_output = train_output[start_idx:end_idx]\n",
    "\n",
    "                # Forward pass\n",
    "                breakpoint()\n",
    "                predictions = self.model(batch_input)\n",
    "                loss = self.loss_fn(predictions, batch_output)\n",
    "\n",
    "                # Backward pass\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Loss: {epoch_loss / num_batches}\")\n",
    "\n",
    "            # Evaluate after each epoch\n",
    "            self.evaluate(test_input, test_output)\n",
    "\n",
    "    def load(self, model_path):\n",
    "        checkpoint = torch.load(model_path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        self.obs_size = checkpoint['obs_size']\n",
    "        self.obs_mean = checkpoint['obs_mean']\n",
    "        self.obs_std = checkpoint['obs_std']\n",
    "\n",
    "    def save(self, model_path):\n",
    "        assert (self.obs_size is not None)\n",
    "\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'obs_size': self.obs_size,\n",
    "            'obs_mean': self.obs_mean,\n",
    "            'obs_std': self.obs_std,\n",
    "        }, model_path)\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = x.to_numpy()\n",
    "            x = self.normalize_input(x, update_obs=inference)\n",
    "            x = torch.from_numpy(x).to(DEVICE)\n",
    "            y_scores = self.model(x)\n",
    "            y_predicted = torch.max(y_scores, 1)[1].to(device=DEVICE)\n",
    "            ret = y_predicted.detach().cpu().numpy()\n",
    "            return ret\n",
    "    def predict_discrete(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = x.to_numpy()\n",
    "            x = self.normalize_input(x, update_obs=inference)\n",
    "            x = torch.from_numpy(x).to(DEVICE)\n",
    "            y_scores = self.model(x)\n",
    "            y_predicted = torch.max(y_scores, 1)[1].to(device=DEVICE)\n",
    "            ret = y_predicted.detach().cpu().numpy()\n",
    "            return y_scores, ret\n",
    "\n",
    "    def predict_cont(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = x.to_numpy()\n",
    "            x = self.normalize_input(x, update_obs=inference)\n",
    "            x = torch.from_numpy(x).to(DEVICE)\n",
    "            y_scores = self.model(x)\n",
    "            y_predicted = torch.max(y_scores, 1)[1].to(device=DEVICE)\n",
    "            ret = y_predicted.double().numpy()\n",
    "            for i in range(len(ret)):\n",
    "                bin_id = ret[i]\n",
    "                if bin_id == 0:  # the first bin is defined differently\n",
    "                    ret[i] = 0.25 * Model.BIN_SIZE\n",
    "                else:\n",
    "                    ret[i] = bin_id * Model.BIN_SIZE\n",
    "            return ret\n",
    "\n",
    "    def evaluate_with_trustee(self, test_input, test_output):\n",
    "        self.model.eval()\n",
    "        pd_input = pd.DataFrame(test_input, columns=self.COLUMNS)\n",
    "        print(min(pd_input[\"trans_time3\"]))\n",
    "        test_output_discretized = self.discretize_output(test_output)\n",
    "        with torch.no_grad():\n",
    "            predictions_prob, class_preds = self.predict_discrete(pd_input)\n",
    "            # Cross-Entropy Loss\n",
    "            cross_entropy_loss = self.loss_fn(predictions_prob, torch.from_numpy(test_output_discretized)).item()\n",
    "            # Print metrics\n",
    "            print(f\"Test Cross-Entropy Loss: {cross_entropy_loss}\")\n",
    "            print(\"Classification Report:\")\n",
    "            print(classification_report(test_output_discretized, class_preds, zero_division=0))\n",
    "            trustee = ClassificationTrustee(expert=model)\n",
    "            trustee.fit(pd_input, test_output_discretized, num_iter=10, num_stability_iter=2, samples_size=0.3, verbose=True)\n",
    "            dt, pruned_dt, agreement, reward = trustee.explain()\n",
    "            dt_y_pred = dt.predict(pd_input)\n",
    "            print(\"Model explanation global fidelity report:\")\n",
    "            print(classification_report(class_preds, dt_y_pred))\n",
    "            print(\"plotting the graph\")\n",
    "            dot_data = tree.export_graphviz(pruned_dt, class_names=[str(i)for i in range(21)], feature_names=model.COLUMNS,filled=True,rounded=True,special_characters=True)\n",
    "            graph = graphviz.Source(dot_data)\n",
    "            print(\"saving the graph\")\n",
    "            fil = graph.render(\"/mnt/md0/laasya/trustee_tree_puffer_pruned\", format=\"png\")\n",
    "            print(\"saved the graph\")\n",
    "\n",
    "    def evaluate(self, test_input, test_output):\n",
    "        self.model.eval()\n",
    "        pd_input = pd.DataFrame(test_input, columns=self.COLUMNS)\n",
    "        with torch.no_grad():\n",
    "            predictions = self.predict_cont(pd_input)\n",
    "            # Print metrics\n",
    "\n",
    "            # Mean Squared Error\n",
    "            mse_loss = mean_squared_error(test_output, predictions)\n",
    "            print(f\"Test Mean Squared Error: {mse_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "103e3dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1639421/1372138520.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mean Squared Error: 0.09133160611336166\n",
      "Test Mean Squared Error: 0.09100634098948815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1639421/1372138520.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mean Squared Error: 0.10092898386961412\n",
      "Test Mean Squared Error: 0.10854561021022033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1639421/1372138520.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n",
      "/tmp/ipykernel_1639421/1372138520.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mean Squared Error: 0.11936656412542476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1639421/1372138520.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n"
     ]
    }
   ],
   "source": [
    "with open('/mnt/md0/laasya/output.pkl', 'rb') as f:\n",
    "    processed_data = pickle.load(f)\n",
    "model = Model()\n",
    "\n",
    "for i, chunk_data in enumerate(processed_data):\n",
    "    input_data = np.array(chunk_data['in'])\n",
    "    output_data = np.array(chunk_data['out'])\n",
    "    if not inference:\n",
    "        print(f\"Training model for future chunk {i + 1}\")\n",
    "        # Train-test split (70% train, 30% test)\n",
    "        train_input, test_input, train_output, test_output = train_test_split(\n",
    "            input_data, output_data, test_size=0.3, random_state=42\n",
    "        )\n",
    "        model.train(train_input, train_output, test_input, test_output)\n",
    "    else:\n",
    "        model.load(\"/mnt/md0/cs190n/py-0-checkpoint-200.pt\")\n",
    "        # the model is available at https://storage.googleapis.com/puffer-models/puffer-ttp/bbr-20221001-1.tar.gz\n",
    "        model.evaluate(input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "99179fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "Test Cross-Entropy Loss: 0.28362520713189976\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96     38867\n",
      "           1       0.88      0.89      0.89     18026\n",
      "           2       0.62      0.51      0.56      1789\n",
      "           3       0.36      0.28      0.31       407\n",
      "           4       0.29      0.15      0.19       150\n",
      "           5       0.23      0.19      0.21        79\n",
      "           6       0.00      0.00      0.00        45\n",
      "           7       0.00      0.00      0.00        22\n",
      "           8       0.06      0.15      0.08        13\n",
      "           9       0.00      0.00      0.00        10\n",
      "          10       0.00      0.00      0.00         4\n",
      "          11       0.00      0.00      0.00         2\n",
      "          12       0.00      0.00      0.00         6\n",
      "          13       0.00      0.00      0.00         3\n",
      "          14       0.00      0.00      0.00         1\n",
      "          15       0.00      0.00      0.00         1\n",
      "          16       0.00      0.00      0.00         2\n",
      "          17       0.00      0.00      0.00         3\n",
      "          18       0.00      0.00      0.00         1\n",
      "          19       0.00      0.00      0.00         2\n",
      "          20       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.92     59446\n",
      "   macro avg       0.16      0.15      0.15     59446\n",
      "weighted avg       0.91      0.92      0.92     59446\n",
      "\n",
      "Initializing training dataset using <__main__.Model object at 0x7f7da733ac20> as expert model\n",
      "Expert model score: 0.15781991653507477\n",
      "Initializing Trustee outer-loop with 2 iterations\n",
      "########## Outer-loop Iteration 0/2 ##########\n",
      "Initializing Trustee inner-loop with 2 iterations\n",
      "########## Inner-loop Iteration 0/10 ##########\n",
      "Sampling 12483 points from training dataset with (41612, 41612) entries\n",
      "Student model 0-0 trained with depth 18 and 354 leaves:\n",
      "Student model score: 0.3583215900242797\n",
      "Student model 0-0 fidelity: 0.3583215900242797\n",
      "########## Inner-loop Iteration 1/10 ##########\n",
      "Sampling 12483 points from training dataset with (45357, 45357) entries\n",
      "Student model 0-1 trained with depth 25 and 364 leaves:\n",
      "Student model score: 0.5019489331365519\n",
      "Student model 0-1 fidelity: 0.5019489331365519\n",
      "########## Inner-loop Iteration 2/10 ##########\n",
      "Sampling 12483 points from training dataset with (49102, 49102) entries\n",
      "Student model 0-2 trained with depth 20 and 335 leaves:\n",
      "Student model score: 0.37441248551700346\n",
      "Student model 0-2 fidelity: 0.37441248551700346\n",
      "########## Inner-loop Iteration 3/10 ##########\n",
      "Sampling 12483 points from training dataset with (52847, 52847) entries\n",
      "Student model 0-3 trained with depth 22 and 307 leaves:\n",
      "Student model score: 0.3948418826488965\n",
      "Student model 0-3 fidelity: 0.3948418826488965\n",
      "########## Inner-loop Iteration 4/10 ##########\n",
      "Sampling 12483 points from training dataset with (56592, 56592) entries\n",
      "Student model 0-4 trained with depth 16 and 304 leaves:\n",
      "Student model score: 0.48690675132085837\n",
      "Student model 0-4 fidelity: 0.48690675132085837\n",
      "########## Inner-loop Iteration 5/10 ##########\n",
      "Sampling 12483 points from training dataset with (60337, 60337) entries\n",
      "Student model 0-5 trained with depth 18 and 314 leaves:\n",
      "Student model score: 0.42219252212962366\n",
      "Student model 0-5 fidelity: 0.42219252212962366\n",
      "########## Inner-loop Iteration 6/10 ##########\n",
      "Sampling 12483 points from training dataset with (64082, 64082) entries\n",
      "Student model 0-6 trained with depth 24 and 313 leaves:\n",
      "Student model score: 0.3650199842402386\n",
      "Student model 0-6 fidelity: 0.3650199842402386\n",
      "########## Inner-loop Iteration 7/10 ##########\n",
      "Sampling 12483 points from training dataset with (67827, 67827) entries\n",
      "Student model 0-7 trained with depth 18 and 317 leaves:\n",
      "Student model score: 0.4135271377621378\n",
      "Student model 0-7 fidelity: 0.4135271377621378\n",
      "########## Inner-loop Iteration 8/10 ##########\n",
      "Sampling 12483 points from training dataset with (71572, 71572) entries\n",
      "Student model 0-8 trained with depth 22 and 343 leaves:\n",
      "Student model score: 0.45149755396135216\n",
      "Student model 0-8 fidelity: 0.45149755396135216\n",
      "########## Inner-loop Iteration 9/10 ##########\n",
      "Sampling 12483 points from training dataset with (75317, 75317) entries\n",
      "Student model 0-9 trained with depth 18 and 294 leaves:\n",
      "Student model score: 0.49334126853984384\n",
      "Student model 0-9 fidelity: 0.49334126853984384\n",
      "########## Outer-loop Iteration 1/2 ##########\n",
      "Initializing Trustee inner-loop with 2 iterations\n",
      "########## Inner-loop Iteration 0/10 ##########\n",
      "Sampling 12483 points from training dataset with (79062, 79062) entries\n",
      "Student model 1-0 trained with depth 16 and 319 leaves:\n",
      "Student model score: 0.3754142939320396\n",
      "Student model 1-0 fidelity: 0.3754142939320396\n",
      "########## Inner-loop Iteration 1/10 ##########\n",
      "Sampling 12483 points from training dataset with (82807, 82807) entries\n",
      "Student model 1-1 trained with depth 20 and 310 leaves:\n",
      "Student model score: 0.4355271641189915\n",
      "Student model 1-1 fidelity: 0.4355271641189915\n",
      "########## Inner-loop Iteration 2/10 ##########\n",
      "Sampling 12483 points from training dataset with (86552, 86552) entries\n",
      "Student model 1-2 trained with depth 29 and 350 leaves:\n",
      "Student model score: 0.48885363127305836\n",
      "Student model 1-2 fidelity: 0.48885363127305836\n",
      "########## Inner-loop Iteration 3/10 ##########\n",
      "Sampling 12483 points from training dataset with (90297, 90297) entries\n",
      "Student model 1-3 trained with depth 21 and 317 leaves:\n",
      "Student model score: 0.4372002708386682\n",
      "Student model 1-3 fidelity: 0.4372002708386682\n",
      "########## Inner-loop Iteration 4/10 ##########\n",
      "Sampling 12483 points from training dataset with (94042, 94042) entries\n",
      "Student model 1-4 trained with depth 19 and 312 leaves:\n",
      "Student model score: 0.47937039910958845\n",
      "Student model 1-4 fidelity: 0.47937039910958845\n",
      "########## Inner-loop Iteration 5/10 ##########\n",
      "Sampling 12483 points from training dataset with (97787, 97787) entries\n",
      "Student model 1-5 trained with depth 19 and 307 leaves:\n",
      "Student model score: 0.718122667226444\n",
      "Student model 1-5 fidelity: 0.718122667226444\n",
      "########## Inner-loop Iteration 6/10 ##########\n",
      "Sampling 12483 points from training dataset with (101532, 101532) entries\n",
      "Student model 1-6 trained with depth 19 and 289 leaves:\n",
      "Student model score: 0.44951149124575035\n",
      "Student model 1-6 fidelity: 0.44951149124575035\n",
      "########## Inner-loop Iteration 7/10 ##########\n",
      "Sampling 12483 points from training dataset with (105277, 105277) entries\n",
      "Student model 1-7 trained with depth 22 and 289 leaves:\n",
      "Student model score: 0.4744810479706779\n",
      "Student model 1-7 fidelity: 0.4744810479706779\n",
      "########## Inner-loop Iteration 8/10 ##########\n",
      "Sampling 12483 points from training dataset with (109022, 109022) entries\n",
      "Student model 1-8 trained with depth 20 and 311 leaves:\n",
      "Student model score: 0.477392463265965\n",
      "Student model 1-8 fidelity: 0.477392463265965\n",
      "########## Inner-loop Iteration 9/10 ##########\n",
      "Sampling 12483 points from training dataset with (112767, 112767) entries\n",
      "Student model 1-9 trained with depth 29 and 341 leaves:\n",
      "Student model score: 0.42973920874552723\n",
      "Student model 1-9 fidelity: 0.42973920874552723\n",
      "Model explanation global fidelity report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96     39368\n",
      "           1       0.91      0.90      0.90     18081\n",
      "           2       0.73      0.65      0.69      1488\n",
      "           3       0.43      0.55      0.49       311\n",
      "           4       0.25      0.41      0.31        76\n",
      "           5       0.49      0.26      0.34        65\n",
      "           6       0.00      0.00      0.00         5\n",
      "           7       0.00      0.00      0.00         5\n",
      "           8       0.39      0.26      0.31        35\n",
      "          20       0.17      0.17      0.17        12\n",
      "\n",
      "    accuracy                           0.93     59446\n",
      "   macro avg       0.43      0.42      0.42     59446\n",
      "weighted avg       0.93      0.93      0.93     59446\n",
      "\n",
      "plotting the graph\n",
      "saving the graph\n",
      "saved the graph\n"
     ]
    }
   ],
   "source": [
    "model.evaluate_with_trustee(input_data, output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff5c7b1",
   "metadata": {},
   "source": [
    "![title](trustee_tree_puffer_pruned.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
